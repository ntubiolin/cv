@article{yusheng2021,
author = {Lin, Yu-Sheng and Liu, Zhe-Yu and Chen, Yu-An and Wang, Yu-Siang and Chang, Ya-Liang and Hsu, Winston H.},
title = {XCos: An Explainable Cosine Metric for Face Verification Task},
year = {2021},
url={https://arxiv.org/abs/2003.05383},
codeurl={https://github.com/ntubiolin/xcos},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3s},
issn = {1551-6857},
doi = {10.1145/3469288},
abstract = {We study the XAI (explainable AI) on the face recognition task, particularly the face verification. Face verification has become a crucial task in recent days and it has been deployed to plenty of applications, such as access control, surveillance, and automatic personal log-on for mobile devices. With the increasing amount of data, deep convolutional neural networks can achieve very high accuracy for the face verification task. Beyond exceptional performances, deep face verification models need more interpretability so that we can trust the results they generate. In this article, we propose a novel similarity metric, called explainable cosine (xCos), that comes with a learnable module that can be plugged into most of the verification models to provide meaningful explanations. With the help of xCos, we can see which parts of the two input faces are similar, where the model pays its attention to, and how the local similarities are weighted to form the output xCos score. We demonstrate the effectiveness of our proposed method on LFW and various competitive benchmarks, not only resulting in providing novel and desirable model interpretability for face verification but also ensuring the accuracy as plugging into existing face recognition models.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
_venue={TOMM},
articleno = {112},
numpages = {16},
keywords = {face recognition, explainable artificial intelligence, face verification, XAI, explainable AI, xCos}
}

@misc{morawski2021nod,
      title={NOD: Taking a Closer Look at Detection under Extreme Low-Light Conditions with Night Object Detection Dataset},
      author={Igor Morawski and Yu-An Chen and Yu-Sheng Lin and Winston H. Hsu},
      year={2021},
      _venue={BMVC},
      url={https://www.bmvc2021-virtualconference.com/assets/papers/1126.pdf},
      codeurl={https://github.com/igor-morawski/NOD},
      abstract = {Recent work indicates that, besides being a challenge in producing perceptually pleasing images, low light proves more difficult for machine cognition than previously thought. In our work, we take a closer look at object detection in low light. First, to support the development and evaluation of new methods in this domain, we present a high-quality large-scale Night Object Detection (NOD) dataset showing dynamic scenes captured on the streets at night. Next, we directly link the lighting conditions to perceptual difficulty and identify what makes low light problematic for machine cognition. Accordingly, we provide instance-level annotation for a subset of the dataset for an in-depth evaluation of future methods. We also present an analysis of the baseline model performance to highlight opportunities for future research and show that low light is a non-trivial problem that requires special attention from the researchers. Further, to address the issues caused by low light, we propose to incorporate an image enhancement module into the object detection framework and two novel data augmentation techniques. Our image enhancement module is trained under the guidance of the object detector to learn image representation optimal for machine cognition rather than for the human visual system. Finally, experimental results confirm that the proposed method shows consistent improvement of the performance on low-light datasets.},
      eprint={2110.10364},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS {9857187,
author = {Igor Morawski and Yu-An Chen and Yu-Sheng Lin and Shusil Dangi and Kai He and Winston H. Hsu},
booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
_venue={CVPRW},
title = {GenISP: Neural ISP for Low-Light Machine Cognition},
url={https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Morawski_GenISP_Neural_ISP_for_Low-Light_Machine_Cognition_CVPRW_2022_paper.pdf},
year = {2022},
volume = {},
issn = {},
pages = {629-638},
abstract = {Object detection in low-light conditions remains a challenging but important problem with many practical implications. Some recent works show that, in low-light conditions, object detectors using raw image data are more robust than detectors using image data processed by a traditional ISP pipeline. To improve detection performance in low-light conditions, one can fine-tune the detector to use raw image data or use a dedicated low-light neural pipeline trained with paired low- and normal-light data to restore and enhance the image. However, different camera sensors have different spectral sensitivity and learning-based models using raw images process data in the sensor-specific color space. Thus, once trained, they do not guarantee generalization to other camera sensors. We propose to improve generalization to unseen camera sensors by implementing a minimal neural ISP pipeline for machine cognition, named GenISP, that explicitly incorporates Color Space Transformation to a device-independent color space. We also propose a two-stage color processing implemented by two image-to-parameter modules that take down-sized image as input and regress global color correction parameters. Moreover, we propose to train our proposed GenISP under the guidance of a pre-trained object detector and avoid making assumptions about perceptual quality of the image, but rather optimize the image representation for machine cognition. At the inference stage, GenISP can be paired with any object detector. We perform extensive experiments to compare our proposed method to other low-light image restoration and enhancement methods in an extrinsic task-based evaluation and validate that GenISP can generalize to unseen sensors and object detectors. Finally, we contribute a low-light dataset of 7K raw images annotated with 46K bounding boxes for task-based benchmarking of future low-light image restoration and low-light object detection.},
keywords = {image sensors;image color analysis;pipelines;detectors;object detection;cameras;cognition},
doi = {10.1109/CVPRW56347.2022.00078},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPRW56347.2022.00078},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}
